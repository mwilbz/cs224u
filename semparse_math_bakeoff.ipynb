{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bake-off: Learning an alien language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Chris Potts\"\n",
    "__version__ = \"CS224u, Stanford, Spring 2018 term\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "0. [Set-up](#Set-up)\n",
    "0. [The doomsday scenario](#The-doomsday-scenario)\n",
    "0. [The data](#The-data)\n",
    "0. [Objective 1: Oracle accuracy](#Objective-1:-Oracle-accuracy)\n",
    "0. [Objective 2: Predictive accuracy](#Objective-2:-Predictive-accuracy)\n",
    "0. [Bake-off submission](#Bake-off-submission)\n",
    "0. [Objective 3: The translation function](#Objective-3:-The-translation-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "0. Make sure the `sys.path.append` value is the path to your local [SippyCup repository](https://github.com/wcmac/sippycup). (Alternatively, you can add SippyCup to your Python path; see one of the teaching team if you'd like to do that but aren't sure how.)\n",
    "\n",
    "0. Make sure that [semparse_math_bakeoff_data.py](semparse_math_bakeoff_data.py) is in the current directory (or available via your Python path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import copy\n",
    "import random\n",
    "import sys\n",
    "sys.path.append('../sippycup')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The doomsday scenario\n",
    "\n",
    "It's an indeterminate time in the future. An alien invasion is imminent.  We have intercepted many of the aliens'  transmissions and begun the process of decoding their language. Luckily, we have found a small database of alien  language statements paired with numbers that seem to be the denotations of  those statements. \n",
    "\n",
    "Linguists, working tirelessly, have translated the numbers into standard arabic notation, but they have made little headway in understanding the meanings of the words and phrases in the statements. Standard bag-of-words classifiers were little help with the high-dimensional output space.\n",
    "\n",
    "You've been called in personally by World President Zahara Jolie-Pitt-Kardashian to complete the translation task. Your goal is to use the available data to induce a lexicon mapping alien words to their associated mathematical concepts. Time is of the essence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are available in `semparse_math_bakeoff_data.py`, which contains two lists of SippyCup `Example` instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semparse_math_bakeoff_data import mathbake_train, mathbake_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scwokt volms sklofg scwokt glarc 3\n",
      "scwokt thouch sherle sniese scwokt sherle 10\n",
      "thouch glarc sniese thouch kugns -1\n",
      "scwokt sherle sklofg thouch volms 8\n",
      "scwokt thouch kugns sklofg fribbs -1\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    ex = mathbake_train[random.randint(0, len(mathbake_train))]\n",
    "    print(ex.input, ex.denotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 1: Oracle accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, the goal should be to create a grammar that can find at least one parse with the correct denotation. With that done, we can rely on features and our training data to find weights that favor the correct parses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other linguists on the team have extracted the vocabulary, and they can say with confidence that the words in the grammar can be classified as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "integers = ['fribbs', 'volms', 'scincs', 'kugns', 'glarc', 'sherle']\n",
    "predicates = ['sniese', 'thouch', 'sklofg', 'scwokt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can begin building a crude grammar on this basis. We'll start with an empty one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created grammar with 0 rules\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from parsing import Grammar\n",
    "\n",
    "# Increasing this value will increase your chances of finding \n",
    "# correct parses, but it will slow everything down.\n",
    "import parsing\n",
    "parsing.MAX_CELL_CAPACITY = 1000\n",
    "\n",
    "gram = Grammar(start_symbol='$E') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we assume that the integers all have their denotations somewhere in the interval [0,5], and we consider every hypothesis of that form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing import Rule, add_rule\n",
    "\n",
    "for w, i in itertools.product(integers, range(len(integers))):\n",
    "    add_rule(gram, Rule('$E', w, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume also that there are unary and binary operators, so we add those combination rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unary connective, as in English \"minus one\":\n",
    "add_rule(gram, Rule('$E', '$UnOp $E', lambda sems: (sems[0], sems[1])))\n",
    "\n",
    "# First stage of binary connective, as in English \"two plus\":\n",
    "add_rule(gram, Rule('$EBO', '$E $BinOp', lambda sems: (sems[1], sems[0])))\n",
    "\n",
    "# Second stage of binary connective, as in English \"(two plus) seven\":\n",
    "add_rule(gram, Rule('$E', '$EBO $E', lambda sems: (sems[0][0], sems[0][1], sems[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the semantic space of the operators is harder. The executor from SippyCup's `arithmetic.py` seems like a reasonable place to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "unary_ops = {\n",
    "    '~': lambda x: -x\n",
    "}\n",
    "\n",
    "binary_ops = {\n",
    "    '+': lambda x, y: x + y,\n",
    "    '-': lambda x, y: x - y,\n",
    "    '*': lambda x, y: x * y\n",
    "}\n",
    "\n",
    "##################################################\n",
    "#### Consider extending one or both ops dicts ####\n",
    "\n",
    "\n",
    "ops = {key: val for key, val in itertools.chain(unary_ops.items(), binary_ops.items())}\n",
    "\n",
    "def execute(semantics):\n",
    "    if isinstance(semantics, tuple):\n",
    "        op = ops[semantics[0]]\n",
    "        args = [execute(arg) for arg in semantics[1:]]\n",
    "        return op(*args)\n",
    "    else:\n",
    "        return semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: Bring in the words in `predicates`, in the form of a set of grammar rules like those we added for the integers. Since you don't yet know whether the predicates are unary or binary, you'll have to add rules that allow for all possible meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "########## Add your operators rules here ######### \n",
    "for predicate in predicates:\n",
    "    for op in unary_ops.keys():\n",
    "        add_rule(gram, Rule('$UnOp', predicate, op))\n",
    "        \n",
    "for predicate in predicates:\n",
    "    for op in binary_ops.keys():\n",
    "        add_rule(gram, Rule('$BinOp', predicate, op))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all is going well, the vast majority of sentences of the alien language should now have a parse with a correct denotation. That is, our oracle accuracy should be at least 80%. (In fact, if it is this high, it is probably 100% but the target sometimes wasn't included in the sample of parses found during search.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing import parse_input\n",
    "\n",
    "def check_oracle_accuracy(grammar=None, examples=mathbake_train, verbose=True):\n",
    "    oracle = 0\n",
    "    for ex in examples:\n",
    "        # All the denotations for all the parses:\n",
    "        dens = [execute(parse.semantics) for parse in gram.parse_input(ex.input)]\n",
    "        if ex.denotation in dens:\n",
    "            oracle += 1\n",
    "        elif verbose:\n",
    "            print(\"=\" * 70)\n",
    "            print(ex.input)\n",
    "            print(set(dens))\n",
    "            print(ex.denotation)\n",
    "    percent_correct = int(round((oracle/float(len(examples)))*100, 0))\n",
    "    print(\"Oracle accuracy: %s / %s (%s%%)\" % (oracle, len(examples), percent_correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max cell capacity 1000 has been hit 1 times\n",
      "Max cell capacity 1000 has been hit 2 times\n",
      "Max cell capacity 1000 has been hit 4 times\n",
      "Max cell capacity 1000 has been hit 8 times\n",
      "Max cell capacity 1000 has been hit 16 times\n",
      "Max cell capacity 1000 has been hit 32 times\n",
      "Max cell capacity 1000 has been hit 64 times\n",
      "Oracle accuracy: 199 / 200 (100%)\n"
     ]
    }
   ],
   "source": [
    "check_oracle_accuracy(grammar=gram, examples=mathbake_train, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "If your oracle accuracy isn't above 80%, then consider expanding the space of operators defined by `ops` and expanding the space of rules accordingly. __There's no guarantee that the alien language uses precisely the operators given by `ops`!__ (Hint: it would be a lot of trouble to deal with operators that could return non-`int` values.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Objective 2: Predictive accuracy\n",
    "\n",
    "Your grammar is now successful in that it finds correct parses and associated denotations for the alien language. However, World President Zahara Jolie-Pitt-Kardashian is unlikely to be impressed, because you can't tell her _which_ denotation is correct, and so you can't induce a translation lexicon either. To address this, we need to find feature weights that are effective at using the training data to identify the best hypotheses allowed by the grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can start with the core features given by `scoring.rule_features`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsing import Parse\n",
    "\n",
    "def collect_rules(parse):\n",
    "    rules = [parse.rule]\n",
    "    for child in parse.children:\n",
    "        if isinstance(child, Parse):\n",
    "            rules += collect_rules(child)\n",
    "            \n",
    "    return rules\n",
    "            \n",
    "\n",
    "def duplicate_definition_features(parse):\n",
    "    features = defaultdict(float)\n",
    "    rules = collect_rules(parse)\n",
    "    definition_rules = [rule for rule in rules if len(rule.rhs) == 1]\n",
    "    unique_rules = {}\n",
    "    duplicates = defaultdict(float)\n",
    "    for rule in definition_rules:\n",
    "        if rule.sem not in unique_rules:\n",
    "            unique_rules[rule.sem] = set()\n",
    "        if unique_rules[rule.sem] and unique_rules[rule.sem] != set(rule.rhs):\n",
    "            duplicates['duplicate_definition_' + str(rule.sem)] += 1\n",
    "            \n",
    "        unique_rules[rule.sem].add(rule.rhs)\n",
    "        \n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "def duplicate_word_features(parse):\n",
    "    features = defaultdict(float)\n",
    "    rules = collect_rules(parse)\n",
    "    definition_rules = [rule for rule in rules if len(rule.rhs) == 1]\n",
    "    unique_rules = {}\n",
    "    duplicates = defaultdict(float)\n",
    "    for rule in definition_rules:\n",
    "        if rule.rhs[0] not in unique_rules:\n",
    "            unique_rules[rule.rhs[0]] = set()\n",
    "        if unique_rules[rule.rhs[0]] and unique_rules[rule.rhs[0]] != set([rule.sem]):\n",
    "                duplicates['duplicate_word_' + str(rule.rhs[0])] += 1\n",
    "            \n",
    "        unique_rules[rule.rhs[0]].add(rule.sem)\n",
    "#         print(unique_rules)\n",
    "        \n",
    "    \n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ex in mathbake_train[1:2]:\n",
    "#     print(ex.input, ex.denotation)\n",
    "#     for parse in gram.parse_input(ex.input)[:1]:\n",
    "#         print(parse.semantics)\n",
    "#         print(second_order_rule_features(parse))\n",
    "#         for rule in collect_rules(parse):\n",
    "#             print(rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scoring import rule_features\n",
    "\n",
    "def second_order_rule_features(parse):\n",
    "    unary_rule_features = rule_features(parse)\n",
    "    keys = list(unary_rule_features.keys())\n",
    "    binary_rule_features = defaultdict(float)\n",
    "    for i in range(len(keys)):\n",
    "        for j in range(i+1, len(keys)):\n",
    "            key1 = keys[i]\n",
    "            key2 = keys[j]\n",
    "            combined_key = key1 + key2 if key1 < key2 else key2 + key1\n",
    "            binary_rule_features[combined_key] = unary_rule_features[key1] * unary_rule_features[key2]\n",
    "            \n",
    "    return binary_rule_features\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from scoring import Model, rule_features\n",
    "from arithmetic import ArithmeticDomain # A source of more feature functions!\n",
    "\n",
    "def arithmetic_features(parse):\n",
    "    domain = ArithmeticDomain()\n",
    "    \n",
    "    features = defaultdict(float)\n",
    "    for feature_fun in (rule_features, domain.operator_precedence_features, duplicate_definition_features,\n",
    "                       duplicate_word_features, second_order_rule_features):\n",
    "        features.update(feature_fun(parse))\n",
    "\n",
    "    \n",
    "    # Consider adding to the features dict based on properties of\n",
    "    # parse and/or parse.semantics. SippyCup's `ArithmeticDomain`\n",
    "    # has a method `operator_precedence_features` that might be\n",
    "    # helpful here, for example.\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: Improve on the features returned by `arithmetic_features`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now can build and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(grammar=gram, feature_fn=arithmetic_features, executor=execute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TO DO__: Improve on the optimizer settings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Running SGD learning on 200 examples with training metric: denotation accuracy\n",
      "\n",
      "iter. 1; err. 257.8074689; AdaGrad mag. 3.3807746; train acc. 0.1200\n",
      "iter. 2; err. 223.4566035; AdaGrad mag. 1.0055351; train acc. 0.2200\n",
      "iter. 3; err. 216.8404834; AdaGrad mag. 0.6163898; train acc. 0.2700\n",
      "iter. 4; err. 215.4343663; AdaGrad mag. 0.5867082; train acc. 0.3050\n",
      "iter. 5; err. 213.4665754; AdaGrad mag. 0.5058648; train acc. 0.3100\n",
      "Max cell capacity 1000 has been hit 2048 times\n",
      "iter. 6; err. 206.8036255; AdaGrad mag. 0.2800108; train acc. 0.4050\n",
      "iter. 7; err. 208.6562397; AdaGrad mag. 0.2806498; train acc. 0.4100\n",
      "iter. 8; err. 204.8498787; AdaGrad mag. 0.2657576; train acc. 0.4900\n",
      "iter. 9; err. 203.4511157; AdaGrad mag. 0.2368601; train acc. 0.4950\n",
      "iter. 10; err. 203.9596883; AdaGrad mag. 0.2085144; train acc. 0.4600\n",
      "iter. 11; err. 201.0182540; AdaGrad mag. 0.2149279; train acc. 0.5300\n",
      "iter. 12; err. 201.9301242; AdaGrad mag. 0.2139970; train acc. 0.5050\n",
      "iter. 13; err. 199.7278339; AdaGrad mag. 0.1782318; train acc. 0.5150\n",
      "iter. 14; err. 199.9408393; AdaGrad mag. 0.1655059; train acc. 0.5050\n",
      "iter. 15; err. 198.5943280; AdaGrad mag. 0.1431196; train acc. 0.5400\n",
      "iter. 16; err. 197.6798962; AdaGrad mag. 0.1353979; train acc. 0.5550\n",
      "iter. 17; err. 195.7895366; AdaGrad mag. 0.1118853; train acc. 0.5800\n",
      "iter. 18; err. 196.1665798; AdaGrad mag. 0.1122388; train acc. 0.5400\n",
      "iter. 19; err. 195.4347960; AdaGrad mag. 0.1190329; train acc. 0.5600\n",
      "iter. 20; err. 195.0552785; AdaGrad mag. 0.1116425; train acc. 0.5700\n",
      "iter. 21; err. 193.4361430; AdaGrad mag. 0.0958256; train acc. 0.5800\n",
      "iter. 22; err. 193.8013558; AdaGrad mag. 0.0974664; train acc. 0.5750\n",
      "iter. 23; err. 192.8719267; AdaGrad mag. 0.0733801; train acc. 0.5900\n",
      "Max cell capacity 1000 has been hit 4096 times\n",
      "iter. 24; err. 191.8532140; AdaGrad mag. 0.0734540; train acc. 0.5850\n",
      "iter. 25; err. 191.5369093; AdaGrad mag. 0.1167127; train acc. 0.5750\n",
      "iter. 26; err. 191.6946958; AdaGrad mag. 0.0785245; train acc. 0.5800\n",
      "iter. 27; err. 191.4244536; AdaGrad mag. 0.0912995; train acc. 0.6000\n",
      "iter. 28; err. 190.0182892; AdaGrad mag. 0.0631957; train acc. 0.6000\n",
      "iter. 29; err. 190.1799078; AdaGrad mag. 0.0678950; train acc. 0.6200\n",
      "iter. 30; err. 190.1027663; AdaGrad mag. 0.0662148; train acc. 0.6000\n",
      "iter. 31; err. 189.6470641; AdaGrad mag. 0.0686421; train acc. 0.6050\n",
      "iter. 32; err. 188.5248641; AdaGrad mag. 0.0516201; train acc. 0.6250\n",
      "iter. 33; err. 188.2646050; AdaGrad mag. 0.0515814; train acc. 0.6200\n",
      "iter. 34; err. 188.1302359; AdaGrad mag. 0.0527446; train acc. 0.6150\n",
      "iter. 35; err. 188.3138947; AdaGrad mag. 0.0566007; train acc. 0.6200\n",
      "iter. 36; err. 187.5720687; AdaGrad mag. 0.0550434; train acc. 0.6250\n",
      "iter. 37; err. 188.0838104; AdaGrad mag. 0.0568558; train acc. 0.6250\n",
      "iter. 38; err. 186.5951640; AdaGrad mag. 0.0400738; train acc. 0.6200\n",
      "iter. 39; err. 188.1870093; AdaGrad mag. 0.0543665; train acc. 0.6250\n",
      "iter. 40; err. 187.1517916; AdaGrad mag. 0.0501415; train acc. 0.6250\n",
      "iter. 41; err. 187.1132732; AdaGrad mag. 0.0413702; train acc. 0.6400\n",
      "iter. 42; err. 187.0251697; AdaGrad mag. 0.0444993; train acc. 0.6500\n",
      "iter. 43; err. 186.5175019; AdaGrad mag. 0.0428264; train acc. 0.6650\n",
      "iter. 44; err. 186.7872063; AdaGrad mag. 0.0439862; train acc. 0.6450\n",
      "iter. 45; err. 186.2878771; AdaGrad mag. 0.0435130; train acc. 0.6300\n",
      "iter. 46; err. 186.0581767; AdaGrad mag. 0.0452812; train acc. 0.6200\n",
      "iter. 47; err. 185.8587263; AdaGrad mag. 0.0381252; train acc. 0.6500\n",
      "iter. 48; err. 185.7075937; AdaGrad mag. 0.0343172; train acc. 0.6600\n",
      "iter. 49; err. 185.5378891; AdaGrad mag. 0.0329981; train acc. 0.6750\n",
      "iter. 50; err. 185.1991138; AdaGrad mag. 0.0406237; train acc. 0.6300\n",
      "\n",
      "Top 20 and bottom 20 feature weights:\n",
      "    0.16\tRule('$BinOp', 'sklofg', '-')Rule('$E', 'kugns', 1)\n",
      "    0.14\t('~', '*')\n",
      "    0.14\tRule('$E', 'scincs', 4)\n",
      "    0.13\tRule('$BinOp', 'sklofg', '-')Rule('$E', 'fribbs', 2)\n",
      "    0.13\t('~', '-')\n",
      "    0.13\tRule('$BinOp', 'sklofg', '-')Rule('$E', 'volms', 3)\n",
      "    0.12\t('~', '+')\n",
      "    0.11\tRule('$E', 'glarc', 0)Rule('$E', 'scincs', 4)\n",
      "    0.11\tRule('$BinOp', 'sklofg', '-')Rule('$E', 'sherle', 5)\n",
      "    0.10\tRule('$BinOp', 'sniese', '+')Rule('$E', 'glarc', 0)\n",
      "    0.10\tRule('$BinOp', 'sklofg', '+')Rule('$UnOp', 'scwokt', '~')\n",
      "    0.10\tRule('$BinOp', 'sklofg', '-')Rule('$E', 'glarc', 0)\n",
      "    0.09\tRule('$E', 'sherle', 2)Rule('$E', 'volms', 4)\n",
      "    0.09\tRule('$E', 'glarc', 0)\n",
      "    0.09\tRule('$E', 'fribbs', 2)Rule('$UnOp', 'thouch', '~')\n",
      "    0.09\tRule('$E', 'glarc', 0)Rule('$E', 'kugns', 1)\n",
      "    0.08\tRule('$E', 'kugns', 1)\n",
      "    0.08\tduplicate_word_sklofg\n",
      "    0.08\tRule('$BinOp', 'sklofg', '-')Rule('$E', 'scincs', 4)\n",
      "    0.08\tRule('$BinOp', 'sklofg', '+')Rule('$BinOp', 'sklofg', '-')\n",
      "     ...\t...\n",
      "   -0.06\tRule('$E', 'scincs', 2)\n",
      "   -0.06\tRule('$E', 'volms', 3)Rule('$UnOp', 'scwokt', '~')\n",
      "   -0.06\tRule('$BinOp', 'sniese', '-')Rule('$E', 'volms', 3)\n",
      "   -0.06\tRule('$BinOp', 'sklofg', '+')Rule('$E', 'sherle', 5)\n",
      "   -0.07\tRule('$E', 'glarc', 0)Rule('$E', 'sherle', 2)\n",
      "   -0.07\tRule('$BinOp', 'sklofg', '-')Rule('$E', 'sherle', 1)\n",
      "   -0.07\t('-', '*')\n",
      "   -0.07\tRule('$BinOp', 'sklofg', '*')Rule('$E', 'kugns', 1)\n",
      "   -0.07\tRule('$BinOp', 'sklofg', '+')Rule('$UnOp', 'thouch', '~')\n",
      "   -0.08\tRule('$BinOp', 'sklofg', '-')Rule('$E', 'sherle', 0)\n",
      "   -0.08\t('-', '+')\n",
      "   -0.08\tRule('$E', 'fribbs', 2)Rule('$UnOp', 'scwokt', '~')\n",
      "   -0.08\tRule('$E', 'scincs', 4)Rule('$E', 'sherle', 0)\n",
      "   -0.08\tduplicate_word_sherle\n",
      "   -0.08\tduplicate_definition_-\n",
      "   -0.08\tduplicate_word_glarc\n",
      "   -0.09\tRule('$BinOp', 'sklofg', '-')Rule('$UnOp', 'scwokt', '~')\n",
      "   -0.11\tRule('$BinOp', 'sklofg', '-')Rule('$BinOp', 'sniese', '+')\n",
      "   -0.17\t('+', '~')\n",
      "   -0.24\t('-', '~')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from learning import latent_sgd\n",
    "from metrics import DenotationAccuracyMetric\n",
    "\n",
    "##################################################\n",
    "#### Consider improving the optimizer settings ###\n",
    "\n",
    "trained_model = latent_sgd(\n",
    "    model, \n",
    "    mathbake_train,\n",
    "    training_metric=DenotationAccuracyMetric(), \n",
    "    T=50,\n",
    "    l2_penalty=.1,\n",
    "    eta=.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, we can evaluate it on the held-out data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Evaluating on 50 Dev examples\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Over 50 examples:\n",
      "\n",
      "denotation accuracy                0.340\n",
      "denotation oracle accuracy         1.000\n",
      "number of parses                   457.680\n",
      "spurious ambiguity                 0.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from experiment import evaluate_model\n",
    "from metrics import denotation_match_metrics\n",
    "\n",
    "evaluate_model(\n",
    "    model=trained_model, \n",
    "    examples=mathbake_dev, \n",
    "    examples_label=\"Dev\",\n",
    "    metrics=denotation_match_metrics(),\n",
    "    print_examples=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bake-off submission\n",
    "\n",
    "1. Enter your \"denotation accuracy\" score (non-oracle) from the above into the bake-off.\n",
    "1. Enter a description of the feature functions and optimization settings you used.\n",
    "\n",
    "Submission URL: https://goo.gl/forms/DCVvih3prRRa06ov2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective 3: The translation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our primary objective was to learn how to translate the alien language into our own language for math (basic arithmetic). To see how well we did, we can look at the weights the classifier learned for the core rule-based features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "\n",
    "def view_lexical_features(weights):\n",
    "    # Get the lexical features:        \n",
    "    feats = [(featname, val) for featname, val in weights.items() \n",
    "             if val > 0.0 and isinstance(featname, str) and featname.startswith('Rule')]\n",
    "    # Get the core parts:\n",
    "    lex = defaultdict(list)\n",
    "    for featname, val in feats:\n",
    "        try:\n",
    "            r = eval(featname)\n",
    "        except:\n",
    "            continue\n",
    "        lex[r.rhs[0]].append((r.sem, val))    \n",
    "    # Restrict to the highest weights for each feature:\n",
    "    for w, vals in lex.items():\n",
    "        maxval = max([x[1] for x in vals])\n",
    "        vals = [x for x in vals if x[1]==maxval]\n",
    "        lex[w] = vals  \n",
    "    # Printout sorted by our own semantic operators:\n",
    "    for featname, vals in sorted(lex.items(), key=(lambda item: str(item[1]))):\n",
    "        for val in vals:\n",
    "            print(\"'%s' means %s (weight %0.02f)\" % (featname, val[0], val[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sniese' means + (weight 0.05)\n",
      "'sklofg' means - (weight 0.05)\n",
      "'glarc' means 0 (weight 0.09)\n",
      "'kugns' means 1 (weight 0.08)\n",
      "'fribbs' means 2 (weight 0.04)\n",
      "'volms' means 3 (weight 0.08)\n",
      "'scincs' means 4 (weight 0.14)\n",
      "'sherle' means 5 (weight 0.07)\n"
     ]
    }
   ],
   "source": [
    "view_lexical_features(trained_model.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Are you correct?__ \n",
    "\n",
    "[Paste in your output from view_lexical_features here to find out!](https://web.stanford.edu/class/cs224u/cgi-bin/mathbake/)\n",
    "\n",
    "(Paste in the entire output as printed in the cell above; the script that checks the input is pretty strict about the formatting.)\n",
    "\n",
    "This isn't part of the bake-off submission. The stakes are higher here!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
